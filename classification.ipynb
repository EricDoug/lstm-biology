{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-17T00:23:08.760000",
     "start_time": "2016-03-17T00:23:08.056000"
    },
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-17T00:23:11.631000",
     "start_time": "2016-03-17T00:23:09.511000"
    },
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category alive: Found 65 files, created 130 trajectories\n",
      "Category fibro: Found 69 files, created 345 trajectories\n",
      "Category plastic: Found 177 files, created 885 trajectories\n"
     ]
    }
   ],
   "source": [
    "# Read all data files of one category.\n",
    "import os\n",
    "\n",
    "trajectories_per_category = {}\n",
    "categories = ['alive', 'fibro', 'plastic']\n",
    "downsample_steps = [2, 5, 5]\n",
    "for category, downsample_step in zip(categories, downsample_steps):\n",
    "    data_dir = 'data/JulianTrajs/' + category\n",
    "    trajectories_per_category[category] = []  # dimensions: trajectory index -> time step -> coordinate (x, y, z)\n",
    "    filenames = os.listdir(data_dir)    \n",
    "    for filename in filenames:\n",
    "        trajectory = np.genfromtxt(os.path.join(data_dir, filename))\n",
    "        for start in range(downsample_step):\n",
    "            end = -(downsample_step - start)\n",
    "            sliced_trajectory = trajectory[start:end:downsample_step]\n",
    "            trajectories_per_category[category].append(sliced_trajectory)\n",
    "    trajectories_per_category[category] = np.array(trajectories_per_category[category])\n",
    "    print \"Category {}: Found {} files, created {} trajectories\".format(category, len(filenames), len(trajectories_per_category[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-17T00:23:12.347000",
     "start_time": "2016-03-17T00:23:12.333000"
    },
    "collapsed": false,
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert to velocities (um / min).\n",
    "for category in trajectories_per_category:\n",
    "    trajectories_per_category[category] = np.diff(trajectories_per_category[category], axis=1) / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-17T00:23:14.180000",
     "start_time": "2016-03-17T00:23:14.153000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize trajectories to [-1, 1] for the LSTM network (it outputs values between -1 and 1 by default)\n",
    "# TODO: Do not use min/max values here but fixed value to generalize to other datasets.\n",
    "min_value = np.min([np.min(traj) for traj in trajectories_per_category.values()])\n",
    "max_value = np.max([np.max(traj) for traj in trajectories_per_category.values()])\n",
    "for category in trajectories_per_category:\n",
    "    trajectories_per_category[category] = np.interp(trajectories_per_category[category], [min_value, max_value], [-1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-17T00:23:14.847000",
     "start_time": "2016-03-17T00:23:14.845000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Split the original trajectories into training/test so we can use the same split for mini trajectories of different sizes.\n",
    "# TODO: Use seed when shuffling so we always get the same training/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-17T00:23:15.675000",
     "start_time": "2016-03-17T00:23:15.671000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_together(*args):\n",
    "    rng_state = np.random.get_state()\n",
    "    for arg in args:\n",
    "        np.random.shuffle(arg)\n",
    "        np.random.set_state(rng_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-17T00:23:17.112000",
     "start_time": "2016-03-17T00:23:17.109000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split(arr, fraction):\n",
    "    index_to_split = int(np.rint(fraction * len(arr)))\n",
    "    return arr[:index_to_split], arr[index_to_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-17T00:23:24.024000",
     "start_time": "2016-03-17T00:23:23.752000"
    },
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 12420 mini trajectories for fibro\n",
      "Created 7020 mini trajectories for alive\n",
      "Created 31860 mini trajectories for plastic\n",
      "Taking 7020 mini trajectories for each category --> Total: 21060\n"
     ]
    }
   ],
   "source": [
    "len_mini_trajectories = 20\n",
    "# TODO: Maybe train on mini trajectories of different sizes, so the nn learns to generalize on arbitrarily long trajectories.\n",
    "step = 5\n",
    "\n",
    "mini_trajectories_per_category = {}\n",
    "category_to_target = {'alive': 0, 'fibro': 1, 'plastic': 2}\n",
    "target_to_category = {0: 'alive', 1: 'fibro', 2: 'plastic'}\n",
    "\n",
    "for category in trajectories_per_category:\n",
    "    \n",
    "    mini_trajectories_per_category[category] = []\n",
    "    \n",
    "    for trajectory in trajectories_per_category[category]:\n",
    "        for i in range(0, len(trajectory) - len_mini_trajectories, step):\n",
    "            mini_trajectories_per_category[category].append(trajectory[i:i+len_mini_trajectories])# - trajectory[i])  # Set start of mini trajectory to origin.\n",
    "            \n",
    "    print 'Created {} mini trajectories for {}'.format(len(mini_trajectories_per_category[category]), category)\n",
    "    \n",
    "# Take the same number of trajectories per category so we train equal on all categories.\n",
    "num_mini_trajectories_per_category = np.min([len(arr) for arr in mini_trajectories_per_category.values()])\n",
    "\n",
    "mini_trajectories = []\n",
    "classes = []\n",
    "for category, mini_traj in mini_trajectories_per_category.items():\n",
    "    np.random.shuffle(mini_traj)\n",
    "    mini_trajectories.extend(mini_traj[:num_mini_trajectories_per_category])\n",
    "    classes.extend([category_to_target[category]] * num_mini_trajectories_per_category)\n",
    "\n",
    "mini_trajectories = np.array(mini_trajectories)\n",
    "classes = np.array(classes)\n",
    "from keras.utils import np_utils\n",
    "targets = np_utils.to_categorical(classes, 3)\n",
    "\n",
    "# TODO: Take different mini trajectories for each epoch to learn on all data, not just a subset.\n",
    "# Or just take the small dataset multiple times for the mini trajectories.\n",
    "\n",
    "shuffle_together(mini_trajectories, classes, targets)\n",
    "\n",
    "print 'Taking {} mini trajectories for each category --> Total: {}'.format(num_mini_trajectories_per_category, len(mini_trajectories))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-17T00:23:25.444000",
     "start_time": "2016-03-17T00:23:25.441000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Split validation dataset here and pass it to fit function via validation_data parameter.\n",
    "test_mini_trajectories, training_mini_trajectories = split(mini_trajectories, 0.1)\n",
    "test_classes, training_classes = split(classes, 0.1)\n",
    "test_targets, training_targets = split(targets, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-03-16T23:23:30.015Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "D:\\Python\\27_32bit\\lib\\site-packages\\theano\\tensor\\signal\\downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# Set up the network.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# TODO: Train with different number of LSTM layers and LSTM layer sizes and compare accuracy.\n",
    "model.add(LSTM(input_dim=3, output_dim=10, return_sequences=True))\n",
    "#model.add(Dropout(0.05))\n",
    "#model.add(LSTM(output_dim=10, return_sequences=True))\n",
    "model.add(LSTM(output_dim=10, return_sequences=False))\n",
    "#model.add(Dropout(0.05))\n",
    "model.add(Dense(output_dim=3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# TODO: Add Dropout and more LSTM layers. \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-03-16T23:23:35.512Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Callback to evaluate accuracy for each class during training. \n",
    "from keras.callbacks import Callback\n",
    "class ClassHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.class_acc = [[], [], []]\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        training_predicted = self.model.predict_classes(training_mini_trajectories, verbose=0)\n",
    "        for which_class in [0, 1, 2]:\n",
    "            training_acc = 1. * np.sum(training_predicted[training_classes == which_class] == which_class) / len(training_predicted[training_classes == which_class])\n",
    "            self.class_acc[which_class].append(training_acc)\n",
    "            \n",
    "classHistory = ClassHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-03-16T23:23:50.818Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Train with mini trajectories of different lengths and compare the network performance. \n",
    "# Make plot with accuracy over epochs for different mini trajectories length.\n",
    "# According to the RBM thesis, the data should be correlated for 3 to 7 (corrected) time steps. \n",
    "# See details for each category in the thesis - can we replicate this with the NN?\n",
    "# TODO: Train with different lengths at the same time. \n",
    "\n",
    "# TODO: Play around with parameters of rmsprop.\n",
    "# TODO: Use sample_weight or class_weight parameter and see if it can fix unequally distributed data. \n",
    "hist = model.fit(training_mini_trajectories, training_targets, \n",
    "                 batch_size=15, nb_epoch=10, \n",
    "                 verbose=1, show_accuracy=True, validation_split=0.05, callbacks=[classHistory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-12T18:20:36.727000",
     "start_time": "2016-03-12T18:20:36.314000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Use validation or test data for this plot?\n",
    "# TODO: Make epochs start at 1 or record accuracy before training.\n",
    "plt.plot(hist.history['acc'], label='Training')\n",
    "plt.plot(hist.history['val_acc'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "#plt.xlim(60, 100)\n",
    "#plt.ylim(0.9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-12T18:21:12.766000",
     "start_time": "2016-03-12T18:21:12.402000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Maybe try setting the weight for class 0 lower so the other classes get trained as well.\n",
    "# TODO: Make this with training dataset.\n",
    "for i, class_acc in enumerate(classHistory.class_acc):\n",
    "    plt.plot(class_acc, '-', label=\"Class {}\".format(i))\n",
    "    \n",
    "#plt.ylim(0, 1)\n",
    "#plt.xlim(50, 70)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-12T18:21:37.843000",
     "start_time": "2016-03-12T18:21:37.584000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test on a random mini trajectory from the test dataset.\n",
    "i = np.random.randint(len(test_mini_trajectories))\n",
    "# TODO: Check if this is correct; most trajectories seem to go to the top right.\n",
    "reconstructed_trajectory = np.cumsum(test_mini_trajectories[i], axis=0) - test_mini_trajectories[i, 0]\n",
    "plt.plot(reconstructed_trajectory[:, 0], reconstructed_trajectory[:, 1])\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "\n",
    "probabilities, = model.predict(test_mini_trajectories[i:i+1])\n",
    "\n",
    "for cat, prob in enumerate(probabilities):    \n",
    "    print 'Class {}: {:.3f}'.format(cat, prob)    \n",
    "        \n",
    "print 'Is Class', test_classes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-03-13T21:06:47.611Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate accuracy for each class. \n",
    "# TODO: Compare this to human performance.\n",
    "\n",
    "# TODO: What does batch_size do in predict? Esp, is the state of the RNN preserved or reset during one batch?\n",
    "# TODO: Maybe use evaluate together with show_accuracy for some of those things here.\n",
    "training_predicted = model.predict_classes(training_mini_trajectories, verbose=0)\n",
    "test_predicted = model.predict_classes(test_mini_trajectories, verbose=0)\n",
    "\n",
    "training_acc = 1. * np.sum(training_predicted == training_classes) / len(training_predicted)\n",
    "test_acc = 1. * np.sum(test_predicted == test_classes) / len(test_predicted)\n",
    "print 'Overall accuracy:               Training: {:.3f}, Test: {:.3f}'.format(training_acc, test_acc)\n",
    "\n",
    "for which_class in [0, 1, 2]:\n",
    "    training_acc = 1. * np.sum(training_predicted[training_classes == which_class] == which_class) / len(training_predicted[training_classes == which_class])\n",
    "    test_acc = 1. * np.sum(test_predicted[test_classes == which_class] == which_class) / len(test_predicted[test_classes == which_class])\n",
    "    print 'Accuracy for class {} ({:^7}): Training: {:.3f}, Test: {:.3f}'.format(which_class, target_to_category[which_class], training_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use only x and y coordinates for classification and compare to accuracy of using x/y/z data.\n",
    "# TODO: Predict trajectory of one specific class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate accuracy for trajectories of different lengths (ie with different numbers of time steps).\n",
    "# TODO: Does keras reset the state after each prediction?\n",
    "# TODO: Make this work for all classes.\n",
    "mini_trajectories_per_length = {}\n",
    "for length in range(2, 30):\n",
    "    mini_trajectories_per_length[length] = []\n",
    "    for trajectory in trajectories_per_category['plastic']:  # TODO: Do this for all categories/for test dataset.\n",
    "        for i in range(0, len(trajectory) - length, 5):\n",
    "            mini_trajectories_per_length[length].append(trajectory[i:i+length])  # Set start of mini trajectory to origin.\n",
    "    mini_trajectories_per_length[length] = np.array(mini_trajectories_per_length[length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = []\n",
    "for length, traj in mini_trajectories_per_length.items():\n",
    "    a = evaluate(traj, 2)\n",
    "    print 'Accuracy for length {}: {:.3f}'.format(length, a)\n",
    "    acc.append(a)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mini_trajectories_per_length.keys(), acc, 'o-')\n",
    "plt.xlabel('Trajectory Length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Make plot that shows how often which class is predicted for different trajectory lengths\n",
    "# (it seems that the network always predicts class 2 for small trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for optimzation:\n",
    "# - mini trajectory length\n",
    "# - mini trajectory step\n",
    "# - output_dim\n",
    "# - number of LSTM layers\n",
    "# - rmsprop parameters (LOOK THESE UP)\n",
    "# - batch_size\n",
    "# - (maybe) class_weight\n",
    "# - activation/internal parameters of LSTM\n",
    "# - GRU etc.\n",
    "\n",
    "# TODO before optimization:\n",
    "# - check out class_weight\n",
    "# - train with different lengths in the same dataset\n",
    "# - x/y vs x/y/z\n",
    "# - get training/validation/test split right (numbers + replicability)\n",
    "# - use all data for learning not just subset (<-> class_weight)\n",
    "# - infrastructure code to run experiments and save data (maybe also for classes/lengths), plots, models (architecture + weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model to file.\n",
    "open('model/my_model_architecture.json', 'w').write(model.to_json())\n",
    "model.save_weights('model/my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load model from file.\n",
    "from keras.models import model_from_json\n",
    "model = model_from_json(open('model/my_model_architecture.json').read())\n",
    "model.load_weights('model/my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
